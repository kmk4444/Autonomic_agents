{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbZz9gU99ZZciuqacYQz3x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/Autonomic_agents/blob/main/Part3_openai_assistant_structure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- OpenAI created Assistant because of benefit of agents. There are four parts which are Assistant, Thread, Message and Run.\n",
        "- Assistant means like agent. It is consisted of Name, Instructions and Tools.\n",
        "- Thanks to thread, we run assistant.\n",
        "- Then, assistant create message like thread_01.\n",
        "- Lastly, it added to the end of the chain."
      ],
      "metadata": {
        "id": "TenyMEBFXet0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Requirements.txt"
      ],
      "metadata": {
        "id": "EwgFa6jlnLqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!touch requirements.txt\n",
        "!echo langchain >> requirements.txt\n",
        "!echo langchain-openai >> requirements.txt\n",
        "!echo langchain-google-genai >> requirements.txt\n",
        "!echo langchainhub >> requirements.txt\n",
        "!echo python-dotenv >> requirements.txt\n",
        "!echo streamlit >> requirements.txt\n",
        "!echo duckduckgo-search >> requirements.txt\n",
        "!echo anthropic >> requirements.txt\n",
        "!echo beautifulsoup4 >> requirements.txt\n",
        "!echo autogenstudio >> requirements.txt\n",
        "!echo crewai >> requirements.txt"
      ],
      "metadata": {
        "id": "X-QOIcBqnMSi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "terminal / bash komutu"
      ],
      "metadata": {
        "id": "wDCB8GCqnPGB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyla2m-MFGpn",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile assistant_helper.py\n",
        "\n",
        "from openai import OpenAI\n",
        "import time\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "#load_dotenv()\n",
        "\n",
        "#my_key_openai = os.getenv(\"openai_apikey\")\n",
        "my_key_openai=\"----\"\n",
        "\n",
        "client = OpenAI(api_key=my_key_openai)\n",
        "\n",
        "assistant_id = \"asst_9PkxJouK0097L2lrUxSznB2U\" #| Python Code Assistant\n",
        "\n",
        "#create a new thread\n",
        "#create a new run\n",
        "#create a new message\n",
        "#take the AI response\n",
        "\n",
        "#create a new thread\n",
        "#create a new run\n",
        "#create a new message\n",
        "#take the AI response\n",
        "\n",
        "def start_new_thread():\n",
        "\n",
        "    thread = client.beta.threads.create()  # This line creates a new thread using the client object.\n",
        "    thread_id = thread.id # This line retrieves the unique identifier (id) of the newly created thread and stores it in the variable thread_id.\n",
        "\n",
        "    return thread_id # The function returns the thread_id, which can be used for further interaction with the thread.\n",
        "\n",
        "\n",
        "def add_message_to_thread(thread_id, prompt):\n",
        "#This line creates a new message within the thread specified by thread_id. The create method takes several arguments:\n",
        "    message = client.beta.threads.messages.create(\n",
        "                thread_id=thread_id, # This specifies the thread where the message will be posted.\n",
        "                role=\"user\", # This sets the role of the message to \"user\", indicating it's coming from the user interacting with the AI.\n",
        "                content=prompt, # This defines the content of the message, which is stored in the prompt variable.\n",
        "            )\n",
        "\n",
        "\n",
        "def execute_run_cycle(thread_id):\n",
        "#This line creates a new \"run\" within the thread. A \"run\" might represent a cycle of interaction between the user and the AI. It takes arguments similar to add_message_to_thread:\n",
        "    run = client.beta.threads.runs.create(\n",
        "            thread_id=thread_id, # Specifies the thread where the run will occur.\n",
        "            assistant_id=assistant_id # This variable is likely defined elsewhere and might specify the AI assistant to be used within this run.\n",
        "        )\n",
        "#This line starts an infinite loop that continues until the break statement is encountered.\n",
        "    while True:\n",
        "#Inside the loop, this line retrieves information about the current run using its thread_id and the previously retrieved run.id.\n",
        "        run = client.beta.threads.runs.retrieve(\n",
        "            thread_id=thread_id,\n",
        "            run_id=run.id\n",
        "        )\n",
        "\n",
        "        if run.completed_at: #  This line checks if the run has been completed by the AI assistant.\n",
        "            #If the run is complete, this line calculates the elapsed time between creation and completion.\n",
        "            elapsed = run.completed_at - run.created_at\n",
        "            elapsed = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed))\n",
        "            print(f\"Run completed in {elapsed}\")\n",
        "            print(\"-\"*100)\n",
        "            break\n",
        "        time.sleep(1)\n",
        "    #Outside the loop, this line retrieves a list of all messages within the thread.\n",
        "    messages = client.beta.threads.messages.list(thread_id=thread_id)\n",
        "    last_message = messages.data[0]\n",
        "\n",
        "    #This line extracts the text content of the AI's response from the retrieved message object.\n",
        "    AI_Response = last_message.content[0].text.value\n",
        "\n",
        "    return AI_Response #The function returns the extracted AI response as text."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcDvsMLvnTLk",
        "outputId": "f02b46c1-2179-4a73-bbe5-f3a7065ed340"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting assistant_helper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_assistant.py\n",
        "\n",
        "import streamlit as st\n",
        "import assistant_helper\n",
        "#Checks if \"messages\" key exists in st.session_state. If not, it creates an empty list for messages and sets thread_id to None. This likely initializes conversation history and thread tracking.\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "    st.session_state.thread_id = None\n",
        "\n",
        "\n",
        "st.set_page_config(page_title=\"Python Kodlama Asistanı\", page_icon=\":speech_balloon:\")\n",
        "st.title(\"Python Kodlama Asistanı\")\n",
        "st.divider()\n",
        "\n",
        "\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "if prompt := st.chat_input(\"Mesajınızı yazınız\"):\n",
        "\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "#Checks if thread_id is already set in st.session_state. If not:\n",
        "  #Calls assistant_helper.start_new_thread to initiate a new thread and stores the ID in thread_id.\n",
        "    with st.spinner(\"Yanıt oluşturuluyor...\"):\n",
        "        if st.session_state.thread_id is None:\n",
        "            st.session_state.thread_id = assistant_helper.start_new_thread()\n",
        "        #Calls assistant_helper.add_message_to_thread to send the user's prompt to the thread.\n",
        "        assistant_helper.add_message_to_thread(thread_id=st.session_state.thread_id, prompt=prompt)\n",
        "        #Calls assistant_helper.execute_run_cycle to process the prompt and retrieve the AI response.\n",
        "        AI_Response = assistant_helper.execute_run_cycle(thread_id=st.session_state.thread_id)\n",
        "\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            st.markdown(AI_Response)\n",
        "\n",
        "    st.session_state.messages.append({\"role\":\"user\", \"content\": prompt})\n",
        "    st.session_state.messages.append({\"role\":\"assistant\", \"content\": AI_Response})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pixiv60ipzR-",
        "outputId": "f75a19bb-bfe1-4b85-d616-3545bb86391e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app_assistant.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app_assistant.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC0yxxpPuWF0",
        "outputId": "d293d5b1-d433-4871-949e-e0e6baadbd0c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 0.509s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 2 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.044s\n",
            "your url is: https://eleven-trees-thank.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}